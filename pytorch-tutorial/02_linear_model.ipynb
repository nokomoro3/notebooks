{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l92fnMTyG2qf"
   },
   "source": [
    "### References\n",
    "- https://jovian.ai/aakashns/01-pytorch-basics\n",
    "- https://jovian.ai/aakashns/02-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l92fnMTyG2qf"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1632324690044,
     "user": {
      "displayName": "Shogo NAKAMURA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjq871bPdbPzEDWuA7GEmzJIZo26eEff3bho2Ijlw=s64",
      "userId": "05714879921574505023"
     },
     "user_tz": -540
    },
    "id": "DjzGBa5Cx5SJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ_y568QH9ge"
   },
   "source": [
    "### gradとは\n",
    "- 微分対象にしたい変数に、requires_grad=Trueをする。\n",
    "- これらを使用した演算結果は、backwardできるように勝手になる。\n",
    "- backwardは誤差逆伝搬のこと。\n",
    "- 機械学習のパラメータは、再急降下法により更新され、最適値を目指す。\n",
    "- loss関数を各パラメータで偏微分した値でパラメータを更新する。\n",
    "  - lossをy、parametersをX、更新後のparametersをX'とすると、更新式は以下となる。\n",
    "  - X'[t] = X[t] - a * dy / dX[t]\n",
    "  - ここで、aはいわゆるlearning_rateである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1632325014553,
     "user": {
      "displayName": "Shogo NAKAMURA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjq871bPdbPzEDWuA7GEmzJIZo26eEff3bho2Ijlw=s64",
      "userId": "05714879921574505023"
     },
     "user_tz": -540
    },
    "id": "r7LkwbB9HSiR",
    "outputId": "8f3fa3d5-8ac1-4780-bf9d-28f6dd63c144"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True) # これで微分対象となる\n",
    "b = torch.tensor(5., requires_grad=True) # これで微分対象となる\n",
    "x, w, b\n",
    "\n",
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWKwfFWVJ0Ag"
   },
   "source": [
    "- くらえ！自動微分！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1632325017396,
     "user": {
      "displayName": "Shogo NAKAMURA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjq871bPdbPzEDWuA7GEmzJIZo26eEff3bho2Ijlw=s64",
      "userId": "05714879921574505023"
     },
     "user_tz": -540
    },
    "id": "0gLhtjBPHdrd",
    "outputId": "91ca8062-3892-4c37-f307-dded1c5ee33f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dy/dx = None, dy/dw = 3.0, dy/db = 1.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.backward()\n",
    "display(f\"dy/dx = {x.grad}, dy/dw = {w.grad}, dy/db = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1K_SfO0LF_f"
   },
   "source": [
    "- ちなみに微分前に、微分を覗くとNoneになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1632325086284,
     "user": {
      "displayName": "Shogo NAKAMURA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjq871bPdbPzEDWuA7GEmzJIZo26eEff3bho2Ijlw=s64",
      "userId": "05714879921574505023"
     },
     "user_tz": -540
    },
    "id": "LetcerHDJ-Nu",
    "outputId": "3bae3799-a48e-495e-b8cc-9d4772ebf3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = None, dy/dw = None, dy/db = None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True) # これで微分対象となる\n",
    "b = torch.tensor(5., requires_grad=True) # これで微分対象となる\n",
    "print(f\"dy/dx = {x.grad}, dy/dw = {w.grad}, dy/db = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形回帰\n",
    "- 何かyを推定する際に、入力情報の線形結合でモデル化する場合、これを線形モデルという。\n",
    "  - モデルとしてはこんな感じ。なんかの係数と入力ベクトルの和で表現されるなら全部線形モデル。\n",
    "    - y' = b + a0 * x0 + a1 * x1 ...\n",
    "  - よく、線形は１次関数とか言われますけど、２次関数も線形モデルです。\n",
    "  - なぜなら、以下のような２次関数も入力の線形結合だから。\n",
    "    - y' = b + a0 * x + a1 * x * x\n",
    "- せっかくなんで、pandas使ってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>温度</th>\n",
       "      <th>降水量</th>\n",
       "      <th>湿度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>67</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>88</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>134</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>96</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    温度  降水量  湿度\n",
       "0   73   67  43\n",
       "1   91   88  64\n",
       "2   87  134  58\n",
       "3  102   43  37\n",
       "4   69   96  70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame([[73, 67, 43, 56, 70], \n",
    "                   [91, 88, 64, 81, 101], \n",
    "                   [87, 134, 58, 119, 133], \n",
    "                   [102, 43, 37, 22, 37], \n",
    "                   [69, 96, 70, 103, 119]], columns=[\"温度\", \"降水量\", \"湿度\", \"apples\", \"oranges\"])\n",
    "display(df[[\"温度\", \"降水量\", \"湿度\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 入力と正解をtensorにする。\n",
    "  - pandas -> numpy -> tensorに変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(df[[\"温度\", \"降水量\", \"湿度\"]].values.astype(np.float32))\n",
    "display(inputs.shape)\n",
    "\n",
    "targets = torch.from_numpy(df[[\"apples\", \"oranges\"]].values.astype(np.float32))\n",
    "display(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- パラメータ初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4612, -2.0137,  0.1208],\n",
       "        [-0.2865, -1.4871,  1.2048]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7938, 0.1327], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "display(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデル定義\n",
    "  - 初期値はランダムなので、この時点で推論しても意味不明である。\n",
    "  - .t()は転置になるらしい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-162.5984,  -68.6069],\n",
       "        [-210.6515,  -79.6907],\n",
       "        [-302.1638, -154.1810],\n",
       "        [-128.3673,  -48.4526],\n",
       "        [-215.8909,  -78.0563]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "preds = model(inputs)\n",
    "display(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 誤差関数定義\n",
    "  - 学習の目標とする、最小化したい指標を定義する。\n",
    "  - .numel()は要素数になるらしい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61499.3828, grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MSE ... Mean squared error\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "loss = mse(preds, targets)\n",
    "display(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 再度、微分じゃ！くらえ！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4612, -2.0137,  0.1208],\n",
       "        [-0.2865, -1.4871,  1.2048]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-23296.0312, -26765.3398, -16075.7773],\n",
       "        [-14771.7910, -17252.3125, -10227.2969]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w) # これは前と一緒\n",
    "display(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 微分を元に再急降下法の式で更新する。\n",
    "- この更新計算時は、微分は不要なので、no_gradというcontextmanagerが必要。\n",
    "- contextmanagerは自分で作成もできる。以下を参考。\n",
    "  - https://qiita.com/QUANON/items/c5868b6c65f8062f5876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e-5がlearning_rateである。\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 更新が終わったら、次の更新に備えて、gradをゼロに戻しておいた方が良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "display(w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 更新後の値で、再度推論をし、誤差を計算しなおして、誤差が減ることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42031.2344, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数回のepochで学習\n",
    "- 複数回実行する形式で振り返ります。\n",
    "- トータルとしてこういう流れが必要です。\n",
    "- 少し見た目もおしゃれにしました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc607ce25d452f96a5bf8e3ac47701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 入力\n",
    "inputs = torch.from_numpy(df[[\"温度\", \"降水量\", \"湿度\"]].values.astype(np.float32))\n",
    "\n",
    "# 出力(正解)\n",
    "targets = torch.from_numpy(df[[\"apples\", \"oranges\"]].values.astype(np.float32))\n",
    "\n",
    "# パラメータ初期化\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "# モデル定義\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "# 誤差関数定義\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "# Train for 100 epochs\n",
    "with tqdm(range(100)) as pbar:\n",
    "    for i in pbar:\n",
    "        time.sleep(0.05)\n",
    "        preds = model(inputs)\n",
    "        loss = mse(preds, targets)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad * 1e-5\n",
    "            b -= b.grad * 1e-5\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "        \n",
    "        pbar.set_description(f\"[loss: {loss:.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predsとtargetsの値を比較してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 67.5870,  77.4543],\n",
       "        [ 93.8722, 111.3141],\n",
       "        [ 75.4067,  97.2682],\n",
       "        [ 82.4721,  77.7926],\n",
       "        [ 86.3177, 114.1300]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "display(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchのビルトインの場合\n",
    "- ここまでやってきたことは、標準のクラスで実現できる。\n",
    "- データ読み込みは、ミニバッチ分割・シャッフルなどをDataLoaderに任せられる。\n",
    "  - ここまでバッチ分割せず、全データでパラメータを更新していた。\n",
    "  - バッチ分割とは、データをある単位で分割してそれぞれでパラメータを更新すること。\n",
    "  - これを普通の再急降下法ではなく、確率的勾配降下法(SGD: Statistical Gradient Descent)と呼ぶ\n",
    "- ちなみにSGDはDeep Learningのブレイクスルー要因のひとつだった気がする。\n",
    "- DataLoaderはgeneratorである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[69., 96., 70.],\n",
       "          [91., 88., 64.]]),\n",
       "  tensor([[103., 119.],\n",
       "          [ 81., 101.]])],\n",
       " [tensor([[102.,  43.,  37.],\n",
       "          [ 87., 134.,  58.]]),\n",
       "  tensor([[ 22.,  37.],\n",
       "          [119., 133.]])],\n",
       " [tensor([[73., 67., 43.]]), tensor([[56., 70.]])]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 入力\n",
    "inputs = torch.from_numpy(df[[\"温度\", \"降水量\", \"湿度\"]].values.astype(np.float32))\n",
    "\n",
    "# 出力(正解)\n",
    "targets = torch.from_numpy(df[[\"apples\", \"oranges\"]].values.astype(np.float32))\n",
    "\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_dl = train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "display([*train_dl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデル定義はnnに準備されている。\n",
    "  - requires_gradも勝手についている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4338, -0.5260, -0.4911],\n",
      "        [ 0.2257, -0.4799, -0.2034]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0199, 0.1831], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルのすべてのパラメータを見たい場合は、こうする。\n",
    "  - generatorなのでlist()か\\[*\\]で展開する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4338, -0.5260, -0.4911],\n",
       "         [ 0.2257, -0.4799, -0.2034]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0199, 0.1831], requires_grad=True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display([*model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 推論\n",
    "  - まだ学習してないパラメータで。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -88.0029,  -24.2425],\n",
       "        [-117.1690,  -34.5301],\n",
       "        [-136.6815,  -56.2867],\n",
       "        [ -85.0134,   -4.9602],\n",
       "        [-114.7797,  -44.5546]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 損失関数も代表的なものは準備されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27585.0996, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "loss = loss_fn(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- またパラメタの更新方式をoptimizerと呼んだりする。\n",
    "- 今回は、learning_rate固定のバッチ分割学習なので、普通のSGDとなる。\n",
    "  - 実際は、learning_rateをかなり工夫して調整する方式がたくさんある。<br>\n",
    "    Adamなどを使うのがデファクトスタンダートである気がする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchのビルトインのまとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86225bca35b454abb32a0a7f801b47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(df[[\"温度\", \"降水量\", \"湿度\"]].values.astype(np.float32))\n",
    "targets = torch.from_numpy(df[[\"apples\", \"oranges\"]].values.astype(np.float32))\n",
    "\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "model = nn.Linear(3, 2)\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    with tqdm(range(num_epochs)) as pbar:\n",
    "    # Repeat for given number of epochs\n",
    "        for epoch in pbar:\n",
    "\n",
    "            # バッチ毎の処理\n",
    "            for xb, yb in train_dl:\n",
    "                time.sleep(0.01)\n",
    "\n",
    "                # 推論\n",
    "                pred = model(xb)\n",
    "\n",
    "                # 誤差計算\n",
    "                loss = loss_fn(pred, yb)\n",
    "\n",
    "                # 微分実行\n",
    "                loss.backward()\n",
    "\n",
    "                # モデルのパラメータが更新される\n",
    "                opt.step()\n",
    "\n",
    "                # パラメータの自動微分値を0にクリア\n",
    "                opt.zero_grad()\n",
    "\n",
    "            pbar.set_description(f\"[loss: {loss.item():.1f}]\")\n",
    "            \n",
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predsとtargetsの値を比較してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.3881,  71.6497],\n",
       "        [ 83.4216, 100.0819],\n",
       "        [115.0784, 132.3938],\n",
       "        [ 27.3767,  44.5321],\n",
       "        [100.2727, 113.6942]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "display(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"nokomoro3/pytorch-tutorial-02-linear-model\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/nokomoro3/pytorch-tutorial-02-linear-model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/nokomoro3/pytorch-tutorial-02-linear-model'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jovian\n",
    "jovian.commit(project='pytorch-tutorial-02-linear-model', filename='02_linear_model.ipynb')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhr7j4wWVrVpU+NTTjiYRp",
   "name": "2021-09-18_kaggle_weekly.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
